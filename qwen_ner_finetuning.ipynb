{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Downloading Data"
   ],
   "metadata": {
    "id": "R4YCQakBxvEO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!apt-get install git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gI1QWmftvc-1",
    "outputId": "6a6b22d2-f8c7-4b87-a5b0-c5691d0facb2"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/SinaLab/ArabicNER.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_5JUQB6vc7d",
    "outputId": "eb110315-e8e8-4ec0-b865-caa3204bd23c"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'ArabicNER'...\n",
      "remote: Enumerating objects: 624, done.\u001b[K\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "remote: Total 624 (delta 29), reused 19 (delta 10), pack-reused 568 (from 1)\u001b[K\n",
      "Receiving objects: 100% (624/624), 295.74 KiB | 4.77 MiB/s, done.\n",
      "Resolving deltas: 100% (370/370), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!cd ArabicNER && cd data && ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9FAQlZ-uEOb",
    "outputId": "fd3e904b-88ee-4e43-e421-463885f2f906"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test.txt  train.txt  val.txt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a7RUVI6t9kG",
    "outputId": "ae77f949-72ab-4d23-c586-c339c8758f0d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "shutil.move(\"/content/ArabicNER/data/\", \"/content/drive/MyDrive/Wojood_NER/\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Bak4e2nqxHbR",
    "outputId": "1d40a497-a8fe-423f-a6c5-416d1189734d"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/Wojood_NER/data'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "id": "4NlYpKTJxy54"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qU datasets"
   ],
   "metadata": {
    "id": "WoqLQw-73S7O"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset, DatasetDict, load_from_disk"
   ],
   "metadata": {
    "id": "SSbk4EVy4w66"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def load_ner_data(file_path):\n",
    "\n",
    "    sentences, labels = [], []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read().strip()\n",
    "        raw_sentences = content.split(\"\\n\\n\")\n",
    "\n",
    "        for raw_sentence in raw_sentences:\n",
    "            words, sentence_labels = [], []\n",
    "            for line in raw_sentence.split(\"\\n\"):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    word = parts[0]\n",
    "                    tag_list = parts[1:]\n",
    "                    words.append(word)\n",
    "                    sentence_labels.append(tag_list)\n",
    "\n",
    "            if words and sentence_labels:\n",
    "                sentences.append(words)\n",
    "                labels.append(sentence_labels)\n",
    "\n",
    "    return sentences, labels"
   ],
   "metadata": {
    "id": "LWbmLKzfxHV8"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences, train_labels = load_ner_data(\"/content/drive/MyDrive/Wojood_NER/data/train.txt\")\n",
    "test_sentences, test_labels = load_ner_data(\"/content/drive/MyDrive/Wojood_NER/data/test.txt\")\n",
    "val_sentences, val_labels = load_ner_data(\"/content/drive/MyDrive/Wojood_NER/data/val.txt\")"
   ],
   "metadata": {
    "id": "YE3S4-MA4sIL"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences[69]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAp2Oh16zYHP",
    "outputId": "32541066-a595-4216-8200-25a258b845c3"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['مراسلة',\n",
       " 'بلدية',\n",
       " 'مدينة',\n",
       " 'البيرة',\n",
       " 'بخصوص',\n",
       " 'مدينة',\n",
       " 'البيرة',\n",
       " 'ونظام',\n",
       " 'التقسيمات',\n",
       " 'الإدارية',\n",
       " 'بتاريخ',\n",
       " '(',\n",
       " '25',\n",
       " '/',\n",
       " '1',\n",
       " '/',\n",
       " '1966',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_labels[69]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZoT5NRAzZfZ",
    "outputId": "8b96f40b-dfab-4b14-dcc6-f253a6ee7c6d"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['B-OCC'],\n",
       " ['I-OCC', 'B-ORG'],\n",
       " ['I-OCC', 'I-ORG', 'B-GPE'],\n",
       " ['I-OCC', 'I-ORG', 'I-GPE'],\n",
       " ['O'],\n",
       " ['B-GPE'],\n",
       " ['I-GPE'],\n",
       " ['O'],\n",
       " ['O'],\n",
       " ['O'],\n",
       " ['B-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['I-DATE'],\n",
       " ['O']]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def show_junk_sentences(sentences, labels):\n",
    "    \"\"\"\n",
    "    Prints sentences that are junk, e.g. just '.'\n",
    "    or ['\"', '$#$'] with O labels.\n",
    "    \"\"\"\n",
    "    for i, (words, tags) in enumerate(zip(sentences, labels)):\n",
    "        if len(words) == 1 and words[0] == \".\":\n",
    "            print(f\"Sentence {i}: {words} | Labels: {tags}\")\n",
    "\n",
    "        elif words == ['\"', '$#$'] and all(tag == [\"O\"] for tag in tags):\n",
    "            print(f\"Sentence {i}: {words} | Labels: {tags}\")"
   ],
   "metadata": {
    "id": "kWLU2AlpM_Gh"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_junk_sentences(train_sentences, train_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NA1NPifOEc0",
    "outputId": "c3a9735b-8c4a-4ced-9b87-7f86ce315da7"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 22: ['.'] | Labels: [['O']]\n",
      "Sentence 81: ['.'] | Labels: [['O']]\n",
      "Sentence 82: ['.'] | Labels: [['O']]\n",
      "Sentence 100: ['.'] | Labels: [['O']]\n",
      "Sentence 171: ['.'] | Labels: [['O']]\n",
      "Sentence 187: ['.'] | Labels: [['O']]\n",
      "Sentence 249: ['.'] | Labels: [['O']]\n",
      "Sentence 300: ['.'] | Labels: [['O']]\n",
      "Sentence 367: ['.'] | Labels: [['O']]\n",
      "Sentence 371: ['.'] | Labels: [['O']]\n",
      "Sentence 422: ['.'] | Labels: [['O']]\n",
      "Sentence 436: ['.'] | Labels: [['O']]\n",
      "Sentence 458: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 495: ['.'] | Labels: [['O']]\n",
      "Sentence 505: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 573: ['.'] | Labels: [['O']]\n",
      "Sentence 597: ['.'] | Labels: [['O']]\n",
      "Sentence 628: ['.'] | Labels: [['O']]\n",
      "Sentence 632: ['.'] | Labels: [['O']]\n",
      "Sentence 637: ['.'] | Labels: [['O']]\n",
      "Sentence 721: ['.'] | Labels: [['O']]\n",
      "Sentence 816: ['.'] | Labels: [['O']]\n",
      "Sentence 841: ['.'] | Labels: [['O']]\n",
      "Sentence 877: ['.'] | Labels: [['O']]\n",
      "Sentence 921: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 922: ['.'] | Labels: [['O']]\n",
      "Sentence 952: ['.'] | Labels: [['O']]\n",
      "Sentence 968: ['.'] | Labels: [['O']]\n",
      "Sentence 978: ['.'] | Labels: [['O']]\n",
      "Sentence 1033: ['.'] | Labels: [['O']]\n",
      "Sentence 1049: ['.'] | Labels: [['O']]\n",
      "Sentence 1082: ['.'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_junk_sentences(test_sentences, test_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MS1H25gtOEUR",
    "outputId": "7034bd75-384e-4e39-96e6-ccafb64c4ccd"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 30: ['.'] | Labels: [['O']]\n",
      "Sentence 97: ['.'] | Labels: [['O']]\n",
      "Sentence 196: ['.'] | Labels: [['O']]\n",
      "Sentence 241: ['.'] | Labels: [['O']]\n",
      "Sentence 249: ['.'] | Labels: [['O']]\n",
      "Sentence 256: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 303: ['.'] | Labels: [['O']]\n",
      "Sentence 312: ['.'] | Labels: [['O']]\n",
      "Sentence 336: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 347: ['.'] | Labels: [['O']]\n",
      "Sentence 351: ['.'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_junk_sentences(val_sentences, val_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SFG-gDHOEQz",
    "outputId": "25a36f60-7168-409c-da28-5cb8dff7f066"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 53: ['\"', '$#$'] | Labels: [['O'], ['O']]\n",
      "Sentence 60: ['.'] | Labels: [['O']]\n",
      "Sentence 117: ['.'] | Labels: [['O']]\n",
      "Sentence 124: ['\"', '$#$'] | Labels: [['O'], ['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_junk(sentences, labels):\n",
    "    \"\"\"\n",
    "    Removes junk sentences:\n",
    "    - Single '.'\n",
    "    - ['\"', '$#$'] with all 'O' labels\n",
    "    Returns cleaned sentences and labels.\n",
    "    \"\"\"\n",
    "    clean_sentences, clean_labels = [], []\n",
    "\n",
    "    for words, tags in zip(sentences, labels):\n",
    "        # Case 1: sentence is just \".\"\n",
    "        if len(words) == 1 and words[0] == \".\":\n",
    "            continue\n",
    "\n",
    "        # Case 2: sentence is ['\"', '$#$'] with all 'O' labels\n",
    "        if words == ['\"', '$#$'] and all(tag == [\"O\"] for tag in tags):\n",
    "            continue\n",
    "\n",
    "        # Keep everything else\n",
    "        clean_sentences.append(words)\n",
    "        clean_labels.append(tags)\n",
    "\n",
    "    return clean_sentences, clean_labels"
   ],
   "metadata": {
    "id": "nuLEExdjP8yE"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences, train_labels = remove_junk(train_sentences, train_labels)\n",
    "test_sentences, test_labels = remove_junk(test_sentences, test_labels)\n",
    "val_sentences, val_labels = remove_junk(val_sentences, val_labels)"
   ],
   "metadata": {
    "id": "sa6jnMy8QYWE"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_sentences_with_dollar(sentences, labels):\n",
    "    \"\"\"\n",
    "    Prints all sentences end with '$#$'.\n",
    "    \"\"\"\n",
    "    for i, (words, tags) in enumerate(zip(sentences, labels)):\n",
    "        if words and (words[-1] == \"$#$\"):\n",
    "            print(f\"Sentence {i}: {words} | Labels: {tags}\")"
   ],
   "metadata": {
    "id": "yyhWFIeLOlQJ"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show_sentences_with_dollar(train_sentences, train_labels)"
   ],
   "metadata": {
    "id": "11JKQa0oOlM4"
   },
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_trailing_dollars(sentences, labels):\n",
    "    \"\"\"\n",
    "    Removes one or more '$#$' tokens from the end of each sentence.\n",
    "    \"\"\"\n",
    "    clean_sentences, clean_labels = [], []\n",
    "\n",
    "    for words, tags in zip(sentences, labels):\n",
    "        # Remove trailing $#$ (could be one or many)\n",
    "        while words and words[-1] == \"$#$\":\n",
    "            words = words[:-1]\n",
    "            tags = tags[:-1]\n",
    "\n",
    "        # Only keep if something remains\n",
    "        if words:\n",
    "            clean_sentences.append(words)\n",
    "            clean_labels.append(tags)\n",
    "\n",
    "    return clean_sentences, clean_labels"
   ],
   "metadata": {
    "id": "WtEqM88qOlJ_"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences, train_labels = remove_trailing_dollars(train_sentences, train_labels)\n",
    "test_sentences, test_labels = remove_trailing_dollars(test_sentences, test_labels)\n",
    "val_sentences, val_labels = remove_trailing_dollars(val_sentences, val_labels)"
   ],
   "metadata": {
    "id": "71_n5PvuOlGh"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_single_word_sentences(sentences, labels):\n",
    "    \"\"\"\n",
    "    Prints all sentences that contain only one word/token.\n",
    "    \"\"\"\n",
    "    for i, (words, tags) in enumerate(zip(sentences, labels)):\n",
    "        if len(words) == 1:  # just one token\n",
    "            print(f\"Sentence {i}: {words} | Labels: {tags}\")"
   ],
   "metadata": {
    "id": "yy7hR0g9OlBz"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_single_word_sentences(train_sentences, train_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtsClFZLS6Ks",
    "outputId": "8dbf74ae-925c-4e29-90cb-d31b7c362f17"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 131: ['shopify'] | Labels: [['O']]\n",
      "Sentence 311: ['الخسرانة'] | Labels: [['O']]\n",
      "Sentence 474: ['المدير'] | Labels: [['B-OCC']]\n",
      "Sentence 750: ['نكشات'] | Labels: [['O']]\n",
      "Sentence 1077: ['\"'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_single_word_sentences(test_sentences, test_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwPTgWk0S6Hc",
    "outputId": "52f131db-92c8-4f31-8bf7-4213a0c2e69f"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 92: ['الأختام'] | Labels: [['O']]\n",
      "Sentence 135: ['المجانية'] | Labels: [['O']]\n",
      "Sentence 162: ['google'] | Labels: [['B-WEBSITE']]\n",
      "Sentence 251: ['animations'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "show_single_word_sentences(val_sentences, val_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-Phb8DXS6FJ",
    "outputId": "6455ba78-2c4a-4fea-fa27-d19a34ce5e27"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 147: ['أصحابه'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_junk_single_word(sentences, labels):\n",
    "    clean_sentences, clean_labels = [], []\n",
    "\n",
    "    for words, tags in zip(sentences, labels):\n",
    "        # Remove if it's exactly ['\"']\n",
    "        if len(words) == 1 and words[0] == '\"':\n",
    "            continue\n",
    "\n",
    "        clean_sentences.append(words)\n",
    "        clean_labels.append(tags)\n",
    "\n",
    "    return clean_sentences, clean_labels"
   ],
   "metadata": {
    "id": "8QrpI5WdS6Ci"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences, train_labels = remove_junk_single_word(train_sentences, train_labels)"
   ],
   "metadata": {
    "id": "mTALMed8Sbue"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "show_single_word_sentences(train_sentences, train_labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JFbvRNETif7",
    "outputId": "9e4b7224-9428-4069-f11b-18da272c75f8"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence 131: ['shopify'] | Labels: [['O']]\n",
      "Sentence 311: ['الخسرانة'] | Labels: [['O']]\n",
      "Sentence 474: ['المدير'] | Labels: [['B-OCC']]\n",
      "Sentence 750: ['نكشات'] | Labels: [['O']]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_duplicates(sentences, labels, split_name=\"\"):\n",
    "    \"\"\"\n",
    "    Removes duplicate (sentence, labels) pairs.\n",
    "    Keeps the first occurrence.\n",
    "    Prints how many were removed.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    new_sentences, new_labels = [], []\n",
    "    removed = 0\n",
    "\n",
    "    for words, tags in zip(sentences, labels):\n",
    "        key = (tuple(words), tuple(tuple(t) for t in tags))  # make hashable\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            new_sentences.append(words)\n",
    "            new_labels.append(tags)\n",
    "        else:\n",
    "            removed += 1\n",
    "\n",
    "    print(f\"[{split_name}] Removed {removed} duplicates (kept {len(new_sentences)})\")\n",
    "\n",
    "    return new_sentences, new_labels"
   ],
   "metadata": {
    "id": "IYFJemwjepPt"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_sentences, train_labels = remove_duplicates(train_sentences, train_labels, \"Train\")\n",
    "test_sentences, test_labels = remove_duplicates(test_sentences, test_labels, \"Test\")\n",
    "val_sentences, val_labels = remove_duplicates(val_sentences, val_labels, \"Val\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yxKbHFceq-4",
    "outputId": "7ef77b63-f3ee-44be-e5e2-0493391a95e7"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Train] Removed 0 duplicates (kept 1081)\n",
      "[Test] Removed 0 duplicates (kept 346)\n",
      "[Val] Removed 0 duplicates (kept 154)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def find_empty_entity_sentences(sentences, labels, return_examples=False, max_examples=10):\n",
    "    \"\"\"\n",
    "    Finds sentences that have no entities (all tags are just 'O').\n",
    "\n",
    "    sentences: list of list of tokens\n",
    "    labels: list of list of list of tags (e.g. [['B-ORG'], ['O'], ...])\n",
    "    \"\"\"\n",
    "    empty_indices = []\n",
    "    examples = []\n",
    "\n",
    "    for i, (words, tag_lists) in enumerate(zip(sentences, labels)):\n",
    "        has_entity = False\n",
    "        for tag_list in tag_lists:\n",
    "            # tag_list is a list like ['I-OCC', 'B-ORG'] or ['O']\n",
    "            if any(tag != \"O\" for tag in tag_list):\n",
    "                has_entity = True\n",
    "                break\n",
    "        if not has_entity:\n",
    "            empty_indices.append(i)\n",
    "            if return_examples and len(examples) < max_examples:\n",
    "                examples.append((i, words, tag_lists))\n",
    "\n",
    "    print(f\"Total sentences: {len(sentences)}\")\n",
    "    print(f\"Empty sentences (no entities): {len(empty_indices)}\")\n",
    "    print(f\"Percentage: {len(empty_indices)/len(sentences)*100:.2f}%\")\n",
    "\n",
    "    if return_examples:\n",
    "        return empty_indices, examples\n",
    "    return empty_indices"
   ],
   "metadata": {
    "id": "cHmac38_mQok"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "empty_idx = find_empty_entity_sentences(train_sentences, train_labels, return_examples=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yy44lt7jmVN8",
    "outputId": "cd0e151e-4506-4f02-be91-5ae8a1c8733e"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences: 1081\n",
      "Empty sentences (no entities): 233\n",
      "Percentage: 21.55%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "empty_idx = find_empty_entity_sentences(test_sentences, test_labels, return_examples=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmAY0QPumVK2",
    "outputId": "86e2d0ae-9262-4659-861f-77e0c007b0d5"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences: 346\n",
      "Empty sentences (no entities): 72\n",
      "Percentage: 20.81%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "empty_idx = find_empty_entity_sentences(val_sentences, val_labels, return_examples=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wql7QozmVH2",
    "outputId": "617ad224-49e9-4fbc-eeed-851e5d949df0"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total sentences: 154\n",
      "Empty sentences (no entities): 23\n",
      "Percentage: 14.94%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def remove_empty_sentences(sentences, labels, num_to_remove, seed=42):\n",
    "    \"\"\"\n",
    "    Removes a specified number of sentences that have no entities.\n",
    "\n",
    "    Args:\n",
    "        sentences (list[list[str]]): List of tokenized sentences.\n",
    "        labels (list[list[list[str]]]): Corresponding labels.\n",
    "        num_to_remove (int): Number of empty sentences to remove.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        (clean_sentences, clean_labels)\n",
    "    \"\"\"\n",
    "    # Step 1: find indices of empty sentences\n",
    "    empty_indices = []\n",
    "    for i, tags in enumerate(labels):\n",
    "        has_entity = any(tag != \"O\" for token_tags in tags for tag in token_tags)\n",
    "        if not has_entity:\n",
    "            empty_indices.append(i)\n",
    "\n",
    "    # Step 2: choose which to remove\n",
    "    random.seed(seed)\n",
    "    to_remove = set(random.sample(empty_indices, min(num_to_remove, len(empty_indices))))\n",
    "\n",
    "    # Step 3: build cleaned lists\n",
    "    clean_sentences, clean_labels = [], []\n",
    "    removed_count = 0\n",
    "    for i, (words, tags) in enumerate(zip(sentences, labels)):\n",
    "        if i in to_remove:\n",
    "            removed_count += 1\n",
    "            continue\n",
    "        clean_sentences.append(words)\n",
    "        clean_labels.append(tags)\n",
    "\n",
    "    print(f\"Requested removal: {num_to_remove}\")\n",
    "    print(f\"Actually removed: {removed_count} (out of {len(empty_indices)} empty sentences)\")\n",
    "\n",
    "    return clean_sentences, clean_labels"
   ],
   "metadata": {
    "id": "BWj_v-QuoOf9"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train_sentences, train_labels = remove_empty_sentences(train_sentences, train_labels, num_to_remove=150)"
   ],
   "metadata": {
    "id": "2PRLRKZHoOdA"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def format_ner_data(sentences, labels):\n",
    "\n",
    "    formatted_data = []\n",
    "\n",
    "    for words, tag_lists in zip(sentences, labels):\n",
    "        sentence_text = \" \".join(words)\n",
    "        entities = []\n",
    "\n",
    "        # We’ll track open entities for each entity type separately\n",
    "        open_entities = {}\n",
    "\n",
    "        for word, tag_list in zip(words, tag_lists):\n",
    "            if not tag_list:  # safety check\n",
    "                continue\n",
    "\n",
    "            for tag in tag_list:\n",
    "                if tag == \"O\":\n",
    "                    # Close any open entities of all types\n",
    "                    to_close = []\n",
    "                    for ent_type, tokens in open_entities.items():\n",
    "                        if tokens:\n",
    "                            entities.append({\n",
    "                                \"entity_value\": \" \".join(tokens),\n",
    "                                \"entity_type\": ent_type\n",
    "                            })\n",
    "                            to_close.append(ent_type)\n",
    "                    for ent_type in to_close:\n",
    "                        open_entities[ent_type] = []\n",
    "                    continue\n",
    "\n",
    "                prefix, entity_type = tag.split(\"-\", 1)\n",
    "\n",
    "                if prefix == \"B\":\n",
    "                    # Close previous entity of the same type\n",
    "                    if entity_type in open_entities and open_entities[entity_type]:\n",
    "                        entities.append({\n",
    "                            \"entity_value\": \" \".join(open_entities[entity_type]),\n",
    "                            \"entity_type\": entity_type\n",
    "                        })\n",
    "                    # Start new entity\n",
    "                    open_entities[entity_type] = [word]\n",
    "\n",
    "                elif prefix == \"I\":\n",
    "                    if entity_type in open_entities and open_entities[entity_type]:\n",
    "                        open_entities[entity_type].append(word)\n",
    "                    else:\n",
    "                        # Malformed case: start fresh\n",
    "                        open_entities[entity_type] = [word]\n",
    "\n",
    "        # Flush any remaining entities\n",
    "        for ent_type, tokens in open_entities.items():\n",
    "            if tokens:\n",
    "                entities.append({\n",
    "                    \"entity_value\": \" \".join(tokens),\n",
    "                    \"entity_type\": ent_type\n",
    "                })\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"text\": sentence_text,\n",
    "            \"entities\": json.dumps({\"story_entities\": entities}, ensure_ascii=False)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(formatted_data)"
   ],
   "metadata": {
    "id": "hDiZZGCqvBBN"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = format_ner_data(train_sentences, train_labels)\n",
    "test_df = format_ner_data(test_sentences, test_labels)\n",
    "val_df = format_ner_data(val_sentences, val_labels)"
   ],
   "metadata": {
    "id": "SNqv1SIJ_IPF"
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[\"text\"][69]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ZN1OrSEO_XfF",
    "outputId": "488dabaf-fc9e-4993-894d-fb5e75f41f84"
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ونحث الدول بصفة خاصة على بذل كل جهد ممكن لمباشرة تحقيقات في الهجمات الموجهة ضد العاملين في مجال الرعاية الصحية والمرافق الطبية ووسائل النقل الطبي وإدانتها كافة بصفتها انتهاكات للقانون الدولي ، ولا سيما القانون الدولي الإنساني .'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 38
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[\"entities\"][69]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "93MtxXV46y_y",
    "outputId": "b5c75800-97ff-48eb-d800-0394c7828e40"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"story_entities\": [{\"entity_value\": \"العاملين في مجال الرعاية الصحية\", \"entity_type\": \"NORP\"}, {\"entity_value\": \"للقانون الدولي\", \"entity_type\": \"LAW\"}, {\"entity_value\": \"القانون الدولي الإنساني\", \"entity_type\": \"LAW\"}]}'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(val_df.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69FmCPZHvRdA",
    "outputId": "27b3751d-bc67-48a4-e37a-2be37abdcb94"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1081, 2)\n",
      "(346, 2)\n",
      "(154, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def count_total_entities(df):\n",
    "    total_entities = 0\n",
    "\n",
    "    for entity_json in df[\"entities\"]:\n",
    "        entity_data = json.loads(entity_json)\n",
    "        total_entities += len(entity_data[\"story_entities\"])\n",
    "\n",
    "    return total_entities"
   ],
   "metadata": {
    "id": "0CuK9bbD_mZW"
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_entity_count = count_total_entities(train_df)\n",
    "test_entity_count = count_total_entities(test_df)\n",
    "val_entity_count = count_total_entities(val_df)\n",
    "\n",
    "print(train_entity_count)\n",
    "print(test_entity_count)\n",
    "print(val_entity_count)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1OoQNWqAGjI",
    "outputId": "be34cca0-3bc5-4397-dfaf-5c45467bcaa5"
   },
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2883\n",
      "870\n",
      "470\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def count_entities(df):\n",
    "    \"\"\"\n",
    "    Count how many entities of each type exist in a DataFrame.\n",
    "    Assumes df['entities'] is JSON string or dict with {\"story_entities\": [...]}\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "\n",
    "    for entity_data in df[\"entities\"]:\n",
    "        # handle JSON string vs dict\n",
    "        if isinstance(entity_data, str):\n",
    "            entity_data = json.loads(entity_data)\n",
    "        for ent in entity_data[\"story_entities\"]:\n",
    "            counter[ent[\"entity_type\"]] += 1\n",
    "\n",
    "    return counter"
   ],
   "metadata": {
    "id": "lITYbqbrfmt4"
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "count_entities(train_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-lpKnj1fqMz",
    "outputId": "055a0097-9824-4b1b-bd97-2fdbef33e9ad"
   },
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ORG': 594,\n",
       "         'PERS': 214,\n",
       "         'OCC': 174,\n",
       "         'GPE': 677,\n",
       "         'CARDINAL': 65,\n",
       "         'DATE': 553,\n",
       "         'ORDINAL': 181,\n",
       "         'LAW': 20,\n",
       "         'NORP': 168,\n",
       "         'EVENT': 107,\n",
       "         'TIME': 10,\n",
       "         'WEBSITE': 21,\n",
       "         'LOC': 33,\n",
       "         'FAC': 30,\n",
       "         'PERCENT': 1,\n",
       "         'MONEY': 8,\n",
       "         'CURR': 10,\n",
       "         'LANGUAGE': 8,\n",
       "         'PRODUCT': 3,\n",
       "         'UNIT': 4,\n",
       "         'QUANTITY': 2})"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "count_entities(test_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlXUOJ1pfwaW",
    "outputId": "0a0c18d5-ab7d-4b4b-fa08-934dc1eb545f"
   },
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'PERS': 75,\n",
       "         'GPE': 219,\n",
       "         'EVENT': 21,\n",
       "         'ORG': 194,\n",
       "         'NORP': 37,\n",
       "         'WEBSITE': 11,\n",
       "         'DATE': 156,\n",
       "         'TIME': 3,\n",
       "         'MONEY': 3,\n",
       "         'CURR': 3,\n",
       "         'CARDINAL': 22,\n",
       "         'OCC': 64,\n",
       "         'FAC': 15,\n",
       "         'ORDINAL': 38,\n",
       "         'LAW': 4,\n",
       "         'LOC': 4,\n",
       "         'PERCENT': 1})"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "count_entities(val_df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LMjjpLJfwW1",
    "outputId": "d5a2ff2e-f796-4202-9946-cd12c523ffec"
   },
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ORG': 87,\n",
       "         'OCC': 33,\n",
       "         'GPE': 117,\n",
       "         'PERS': 42,\n",
       "         'DATE': 84,\n",
       "         'UNIT': 1,\n",
       "         'QUANTITY': 1,\n",
       "         'PERCENT': 1,\n",
       "         'WEBSITE': 3,\n",
       "         'CARDINAL': 20,\n",
       "         'NORP': 25,\n",
       "         'TIME': 1,\n",
       "         'FAC': 6,\n",
       "         'ORDINAL': 25,\n",
       "         'MONEY': 2,\n",
       "         'CURR': 3,\n",
       "         'EVENT': 12,\n",
       "         'LOC': 7})"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> The dataset is highly imbalanced"
   ],
   "metadata": {
    "id": "LaNh8SnJWivD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_path = \"/content/drive/MyDrive/Wojood_NER/csv_files/train.csv\"\n",
    "test_path = \"/content/drive/MyDrive/Wojood_NER/csv_files/test.csv\"\n",
    "val_path = \"/content/drive/MyDrive/Wojood_NER/csv_files/val.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False, encoding=\"utf-8-sig\")\n",
    "test_df.to_csv(test_path, index=False, encoding=\"utf-8-sig\")\n",
    "val_df.to_csv(val_path, index=False, encoding=\"utf-8-sig\")"
   ],
   "metadata": {
    "id": "SkKZqTebMjqW"
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(val_df.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBA8X9RWGlrd",
    "outputId": "5e70054e-8842-40c2-c860-53b321efbc17"
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1081, 2)\n",
      "(346, 2)\n",
      "(154, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "EntityType = Literal[\n",
    "    \"PERS\", \"NORP\", \"OCC\", \"ORG\", \"GPE\", \"LOC\", \"FAC\", \"EVENT\",\n",
    "    \"DATE\", \"TIME\", \"CARDINAL\", \"ORDINAL\", \"PERCENT\", \"LANGUAGE\",\n",
    "    \"QUANTITY\", \"WEBSITE\", \"UNIT\", \"LAW\", \"MONEY\", \"PRODUCT\", \"CURR\"\n",
    "]\n",
    "\n",
    "class NEREntity(BaseModel):\n",
    "    entity_value: str = Field(..., description=\"The actual named entity found in the text.\")\n",
    "    entity_type: EntityType = Field(..., description=\"The entity type\")\n",
    "\n",
    "class NERData(BaseModel):\n",
    "    story_entities: List[NEREntity] = Field(..., description=\"A list of entities found in the text.\")"
   ],
   "metadata": {
    "id": "ZEu21oRNGzAz"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_pydantic_format(entities_str):\n",
    "    entities_dict = json.loads(entities_str)\n",
    "    story_entities = [NEREntity(**entity) for entity in entities_dict[\"story_entities\"]]\n",
    "    return NERData(story_entities=story_entities).model_dump_json()\n",
    "\n",
    "train_df[\"entities\"] = train_df[\"entities\"].apply(convert_to_pydantic_format)\n",
    "test_df[\"entities\"] = test_df[\"entities\"].apply(convert_to_pydantic_format)\n",
    "val_df[\"entities\"] = val_df[\"entities\"].apply(convert_to_pydantic_format)"
   ],
   "metadata": {
    "id": "mBMEHTvPJD9n"
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df['text'][69]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "PwMQUEy1dlM2",
    "outputId": "c9547365-a260-454f-f3d9-f25b3cb1ed2b"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'ونحث الدول بصفة خاصة على بذل كل جهد ممكن لمباشرة تحقيقات في الهجمات الموجهة ضد العاملين في مجال الرعاية الصحية والمرافق الطبية ووسائل النقل الطبي وإدانتها كافة بصفتها انتهاكات للقانون الدولي ، ولا سيما القانون الدولي الإنساني .'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 51
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(train_df['entities'][69])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uxy_X6-dsWS",
    "outputId": "b41a13f3-0123-4da0-9ec3-b33d8d565dee"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'story_entities': [{'entity_value': 'العاملين في مجال الرعاية الصحية',\n",
       "   'entity_type': 'NORP'},\n",
       "  {'entity_value': 'للقانون الدولي', 'entity_type': 'LAW'},\n",
       "  {'entity_value': 'القانون الدولي الإنساني', 'entity_type': 'LAW'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuning Data"
   ],
   "metadata": {
    "id": "_MiI0HzpZCtB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ],
   "metadata": {
    "id": "zPgjvh3fyVpo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "5853805418e347d4a8f673c9febd1864",
      "f644a7541e9146e99b7ce5f0bf115a66",
      "0a4e554876334d8bb8b17b6bc66f3bf3",
      "9ad346953b0346f0b8ae490361959208",
      "c40d13cc14434df0832e4d7d25bc1d36",
      "3d8774d5d7f8475da4693fd361334f71",
      "91022adfdaea4029935fdbc8dca48577",
      "80f90ecc58fe448da1f0d9e0816ba9a4",
      "d6e9f450ebda4fa6b756cc81c8d35666",
      "7657aad37cf7401fa8499cbfd95098a9",
      "e7cc8ea6754b4d6e9bff50ec0a30d5db",
      "a592f8fc9e86480b97040d461955efbe",
      "2eb5ea03ebd74892a6f452cfdabe20cd",
      "f01869ffca4e4033862d65a9095f6e77",
      "e5ce3a27078b407d88d6d6c7c5dfe4a6",
      "a8b3f0f112ca4aff89ffc1fccd9867f2",
      "d7c3f7dada33492a869db675d79131be",
      "4fdf78183eb849449b01d251d25d1180",
      "5d09543b77c749f9a2315d689290caa4",
      "c47f0ea090f54fc1b69cee00732ac0e1",
      "ffa769f9e8c6459b91f4150d1ed74d17",
      "b63e9821fa504c3cacfbcacc37e152bc",
      "80aa3dcaceff42c0a3c6ab2fe2f6e1e0",
      "a8756088c7944eee8ccf5cd6ba8f354c",
      "2f8eaaf1bf3c4ddb8864a3e5cffc5e86",
      "ac365774c88d4ef2ae5cb25c58be3d90",
      "90c1d5db9c234182a9cc6ffd4582bee5",
      "800729c2a05945dd82fd17b073d885cf",
      "a7ec902e71dc4ef0a323f742d699cea9",
      "dc65fb55a97e4db3bb4815955f06fa80",
      "394a6b574ec14453a5cffa3e7b860e8f",
      "761a55dda0974c3cb65e4d85600669f7",
      "ff6bddc797b144e78db528807a5c997d",
      "43b05081d8cf40e2aa7d15a256a455a6",
      "705cf6818e214bce8662e954ab0af037",
      "d3d776b7f3064b5bbb3a7f9cb6bf8c00",
      "19bed2bc26854e658d94bf5a66269594",
      "1a75481f59f64337963ed0a46a6ede1e",
      "f65b09d8603748609f9a75a4c7470638",
      "24529e07256a453ea81431fd742a2d86",
      "8721424c026c4efcabbd707091d42f3f",
      "6efd3703bf6f46509b38fad655d30fff",
      "cfe5644a825544438c73d341d632278c",
      "53240cef76ad4c748fccd060155b3d05"
     ]
    },
    "outputId": "361d2be2-d082-414e-d678-4b3be8f4be91"
   },
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def data_format(row):\n",
    "    formatting_prompt = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"You are an advanced NLP entity extraction assistant.\",\n",
    "                \"Your task is to extract named entities from Arabic text according to a given Pydantic schema.\",\n",
    "                \"Ensure that the extracted entities exactly match how they appear in the text, without modifications.\",\n",
    "                \"Follow the schema strictly, maintaining the correct entity types and structure.\",\n",
    "                \"Output the extracted entities in JSON format, structured according to the provided Pydantic schema.\",\n",
    "                \"Do not add explanations, introductions, or extra text, Only return the formatted JSON output.\"\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"## Text:\",\n",
    "                row['text'].strip(),\n",
    "                \"\",\n",
    "                \"## Pydantic Schema:\",\n",
    "                json.dumps(\n",
    "                    NERData.model_json_schema(), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "                \"\",\n",
    "                \"## Text Entities:\",\n",
    "                \"```json\"\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": row[\"entities\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        formatting_prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "id": "9G6XpuQVZGPP"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[\"text\"] = train_df.apply(data_format, axis=1)\n",
    "test_df[\"text\"] = test_df.apply(data_format, axis=1)\n",
    "val_df[\"text\"] = val_df.apply(data_format, axis=1)"
   ],
   "metadata": {
    "id": "bgdywZuLZGL8"
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_final = train_df[[\"text\"]]\n",
    "test_final = test_df[[\"text\"]]\n",
    "val_final = val_df[[\"text\"]]"
   ],
   "metadata": {
    "id": "0XhJUZ3qZGJv"
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_final['text'][0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "-T0ns1XV3CP3",
    "outputId": "6083afdb-1474-4f8b-fb35-6197bf86d30a"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are an advanced NLP entity extraction assistant.\\nYour task is to extract named entities from Arabic text according to a given Pydantic schema.\\nEnsure that the extracted entities exactly match how they appear in the text, without modifications.\\nFollow the schema strictly, maintaining the correct entity types and structure.\\nOutput the extracted entities in JSON format, structured according to the provided Pydantic schema.\\nDo not add explanations, introductions, or extra text, Only return the formatted JSON output.<|im_end|>\\n<|im_start|>user\\n## Text:\\nفقد حرصت روسيا على تعطيل مشاريع قرارات مختلفة في مجلس الأمن ضد الأسد وتدخلت عسكرياً لقلب الكفة لصالحه ،\\n\\n## Pydantic Schema:\\n{\\n  \"$defs\": {\\n    \"NEREntity\": {\\n      \"properties\": {\\n        \"entity_value\": {\\n          \"description\": \"The actual named entity found in the text.\",\\n          \"title\": \"Entity Value\",\\n          \"type\": \"string\"\\n        },\\n        \"entity_type\": {\\n          \"description\": \"The entity type\",\\n          \"enum\": [\\n            \"PERS\",\\n            \"NORP\",\\n            \"OCC\",\\n            \"ORG\",\\n            \"GPE\",\\n            \"LOC\",\\n            \"FAC\",\\n            \"EVENT\",\\n            \"DATE\",\\n            \"TIME\",\\n            \"CARDINAL\",\\n            \"ORDINAL\",\\n            \"PERCENT\",\\n            \"LANGUAGE\",\\n            \"QUANTITY\",\\n            \"WEBSITE\",\\n            \"UNIT\",\\n            \"LAW\",\\n            \"MONEY\",\\n            \"PRODUCT\",\\n            \"CURR\"\\n          ],\\n          \"title\": \"Entity Type\",\\n          \"type\": \"string\"\\n        }\\n      },\\n      \"required\": [\\n        \"entity_value\",\\n        \"entity_type\"\\n      ],\\n      \"title\": \"NEREntity\",\\n      \"type\": \"object\"\\n    }\\n  },\\n  \"properties\": {\\n    \"story_entities\": {\\n      \"description\": \"A list of entities found in the text.\",\\n      \"items\": {\\n        \"$ref\": \"#/$defs/NEREntity\"\\n      },\\n      \"title\": \"Story Entities\",\\n      \"type\": \"array\"\\n    }\\n  },\\n  \"required\": [\\n    \"story_entities\"\\n  ],\\n  \"title\": \"NERData\",\\n  \"type\": \"object\"\\n}\\n\\n## Text Entities:\\n```json<|im_end|>\\n<|im_start|>assistant\\n{\"story_entities\":[{\"entity_value\":\"روسيا\",\"entity_type\":\"ORG\"},{\"entity_value\":\"مجلس الأمن ضد الأسد\",\"entity_type\":\"ORG\"},{\"entity_value\":\"الأسد\",\"entity_type\":\"PERS\"}]}<|im_end|>\\n<|im_start|>assistant\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 58
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = Dataset.from_pandas(train_final)\n",
    "test_dataset = Dataset.from_pandas(test_final)\n",
    "val_dataset = Dataset.from_pandas(val_final)"
   ],
   "metadata": {
    "id": "ck1ABgXo3Z7J"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": val_dataset})\n",
    "dataset"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl1Qdr373gQt",
    "outputId": "772b6a1d-16ea-4b3b-faf4-5759d2b618b0"
   },
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1081\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 346\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 154\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset.save_to_disk(\"/content/drive/MyDrive/Wojood_NER/datasets/train_dataset\")\n",
    "test_dataset.save_to_disk(\"/content/drive/MyDrive/Wojood_NER/datasets/test_dataset\")\n",
    "val_dataset.save_to_disk(\"/content/drive/MyDrive/Wojood_NER/datasets/val_dataset\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "8c7fc562679442f3b3820c6f661f25bb",
      "3cd047d6488f4c77b2c8c94ee2cfe193",
      "4c51c60e20fa4c3fbc23a221a60b87c5",
      "396749401a6e412fa6b88ec1a0238747",
      "c32703adb1964e819b87570a43d1d772",
      "4d647ca487e54da8a412179c0321d52f",
      "6c00a8b54c9447119f1874829918c6e3",
      "71ca44b885f84fa5a91d46acc0e196db",
      "c374792fa94d4144ab1e603cc0d94075",
      "5efa96935f8949b58cc0fe65411ba005",
      "faeb690c925a43cb905269a15e3d7c94",
      "951560caf8724171a11119009c021bc7",
      "cae4b4cf6ede459292f4b5066ed0e84b",
      "d322d491b9f54e4a8cf956a31a1328de",
      "5c9d4080d4bb455691df29d3321cab53",
      "dcdc2fb753ab4d80842217b78157e4f6",
      "1571671520b74942b89509d821a9dd57",
      "a1153f2dfca7438ca5d2406258d26d67",
      "72b29b73078c44f2807a76a22c273ebf",
      "13d93639382b45baa670b40d1bc9b14f",
      "2cd43c60613c4e7b9b48bedca676015c",
      "a8941bb7ea0241ed97196de32a5f32f3",
      "e07e699b3fb54f498f78a1af5adc2b2f",
      "b890da6b5bbe429c98a54042e59f8e28",
      "22c3cef8686d43b7a140d85d9e787363",
      "8d912e8a9b5247eda179ef8fe17f8c96",
      "c6d6ec83e3c2481ea593c3e78cefc246",
      "bd5202a75c514bcfb52f0ff68390d7aa",
      "3ac702b8d7d941559844d81e7772302a",
      "ffeb9c8e0b1e4d46ad6c7acfa8014cad",
      "df8de805700d440a958ac1fd35669b33",
      "0c53bcb294a04999b4789e64189bfa6c",
      "74e576361f1b4de180c5325b243fe0bc"
     ]
    },
    "id": "UwpbFQJK4KLP",
    "outputId": "aec997a3-dd3d-4a92-acc4-f0a5020273c9"
   },
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1081 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset.save_to_disk(\"/content/drive/MyDrive/Wojood_NER/dataset/\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d0c3dd123dba4043a90d19f63d990ca3",
      "1e91cdb0cc194aa2b9fa7db56565ebe9",
      "6325c8e309fc4d1c8c4519164f84c867",
      "d03750fbf1df4c609602d6c15bdaf3f7",
      "5f5dceca7388464e9fe0ec172246b97c",
      "9659a47a54b3470eaf6aebc8556817c0",
      "2c0587d973254e4dbf16892385fd28e5",
      "40f803f82fd84d70828b3e616b9fa258",
      "d653cb27c3724be788b82b105c6ec461",
      "8236215da47c4fe78ef99054eb3e88b6",
      "eac6ad4d93a44ac9be3244c143cd7084",
      "c0e3b80f83be45d2a050debbc5afc081",
      "25b16b5ee16b498986d579f50c143960",
      "618b50f675044ae4bd2e9add7a79e6ea",
      "2070758823f648f38ae25b40c7159116",
      "883307a730b74b49a7a688ccddb2d6f0",
      "6bf227c7531a4e18b6fff385b91c85d8",
      "8f0899321f064809b089a12a6c3a8c53",
      "db4e3a855abf456fa465134cc606a89b",
      "2466cef9913240919a636a82d1b0b337",
      "4fc63cbcb66c4f5d81a9ba3ae80e2334",
      "4a5c68c38ad841539e5328effd904d6e",
      "15646bb0baea47e3a025fda58da223e2",
      "177fcbb7282d41f5861ae582ca4c2ff3",
      "a2016f2ac5fe49ec9a83dd160524525c",
      "7ba81fb7c8e54ed099c97ab814183c31",
      "454dc0ff7945427cb29a9bfcdf541d20",
      "4dea6610e5274964842b9c8bbc801a2c",
      "14ce2e31af1f42edb624997278b00711",
      "7f6560c65f7a40a3ad36e43ecc58bfcc",
      "0691d284036a4a77b0a03a0b52e5356c",
      "c0c29e02560943509808444cb010ca62",
      "1eb7435638fa42389a5e5847a38e3966"
     ]
    },
    "id": "YDrz5g5GSlqQ",
    "outputId": "493b7fd6-4958-4b8b-ba75-14de254199e8"
   },
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1081 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "id": "bWdFTorPM9_l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_model_id = \"Qwen/Qwen2.5-0.5B-Instruct\""
   ],
   "metadata": {
    "id": "vKZvDihgZVhM"
   },
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "metadata": {
    "id": "Y-CkrccjZVd2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "c2695b8e199d488691f99e58a9632149",
      "40c7bcd0d3d14d8189462a251b4cc6b7",
      "4274f57932704b4ab07877ba4041a524",
      "40d912f2ed8144cea5b0904ba8b4c94c",
      "5b03928dcfb3427d91eb8f587c6b5021",
      "06c85100adf24c50b1799d2cf173d704",
      "c5d65d5e6cdb4372a9c6efc0dc9002b1",
      "5002f389dcb44470ab6614e9f9f0b028",
      "dd416da5d5d945d59c8620561e24057b",
      "d4cab601f4f746f39890abcd0f8fcbcb",
      "a16c821a427641fb84167bf38b7644cb",
      "8befb19ad92a4460bfb7ac144114f87e",
      "15b8c2c54a264a539a6085c208256850",
      "d3851d01ce8144a7befdd75e933b6b18",
      "988f8187503542b7a03faeb577516711",
      "a9cc9fe0fe4f4795ad6e1b6ebf521643",
      "af79b93fe2a24f32a2a7bb915e558d95",
      "8359d945cc4945a197fd10bce2a8d00f",
      "d22a533ab74a405b8aa308dbcc72fcd9",
      "18e79c52bf4c456b99003a1fc185de79",
      "db8bb0b9d6c74589a9b9c6cef3637986",
      "4d8d3ab548824fbd94c001c1b2134c79",
      "9d9cc9eb78fa44dfbf45da50fdd3a557",
      "2126041b5b834015abfd0cd6bd81c98e",
      "8e453541ed9745e9a8afe7d5e78c677c",
      "0ebda49c62eb47548f85411fd61f75fe",
      "e967e205c2634630a5866a9279250c8e",
      "f26f4b2be425412da2a4ddb2656539f7",
      "35c88980b2e74d75b6704ae4cdf29a30",
      "6325a171922240119c8534379171f1ed",
      "10fc89ff089a4393b22649d7e8fdf455",
      "9fceafb4aba54448a60e0307053cab59",
      "a03f078c6dd6491a997c9b056a53b8a4"
     ]
    },
    "outputId": "ce6dacc3-f85d-438f-971b-a8dd8f2e1129"
   },
   "execution_count": 64,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_entities_from_story(story, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract named entities from Arabic text using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        story (str): The Arabic text to extract entities from\n",
    "        model: The fine-tuned model\n",
    "        tokenizer: The tokenizer\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response containing extracted entities in JSON format\n",
    "    \"\"\"\n",
    "    # Create the messages for entity extraction\n",
    "    entities_extraction_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"You are an advanced NLP entity extraction assistant.\",\n",
    "                \"Your task is to extract named entities from Arabic text according to a given Pydantic schema.\",\n",
    "                \"Ensure that the extracted entities exactly match how they appear in the text, without modifications.\",\n",
    "                \"Follow the schema strictly, maintaining the correct entity types and structure.\",\n",
    "                \"Output the extracted entities in JSON format, structured according to the provided Pydantic schema.\",\n",
    "                \"Do not add explanations, introductions, or extra text, Only return the formatted JSON output.\"\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"## Text:\",\n",
    "                story.strip(),\n",
    "                \"\",\n",
    "                \"## Pydantic Schema:\",\n",
    "                json.dumps(\n",
    "                    NERData.model_json_schema(), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "                \"\",\n",
    "                \"## Text Entities:\",\n",
    "                \"```json\"\n",
    "            ])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        entities_extraction_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        top_k=None,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "id": "WfKcPLWOZXhv"
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "المحكمة العليا الإسرائيلية تقرر استمرار عمل الكسّارات الإسرائيلية في الضفة الغربية بتاريخ ( 29 / 12 / 2011 ) .\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "id": "WMX-fvH0M8kH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "outputId": "9b5eb16b-4c9e-4903-fe7d-1f710b90bfb5"
   },
   "execution_count": 66,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"entity_value\": \"المحكمة العليا الإسرائيلية\",\\n  \"entity_type\": \"ORG\"\\n}\\n```'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 66
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "MA437XJGXavz",
    "outputId": "f8d50ad9-8269-4ec2-edf7-7398ce53c107"
   },
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "error",
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-161840406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "مضابط بلدية نابلس عام ( 1308 ) هجري مضبط رقم 435 .\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "S6KuwhtcXeqc",
    "outputId": "37fe0cfd-8991-428e-a33b-a093d2eaff0b"
   },
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"entity_value\": \"بلدية نابلس عام (1308) هجري مضبط رقم 435\",\\n  \"entity_type\": \"LOCATION\"\\n}\\n```json'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 68
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "yO4Kb8ZeXarJ",
    "outputId": "8053be92-537d-4683-91a7-591924f0a515"
   },
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "error",
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-161840406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "تتجه أنظار \"وول ستريت\" إلى إنفيديا، سهم شركة الرقائق الرائدة الذي كان رمزاً لطفرة الذكاء الاصطناعي والمفضل لدى المستثمرين الأفراد، قبل تقرير أرباح الشركة، بعد الجرس يوم الأربعاء.\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ykdghj9xeBRM",
    "outputId": "f404eba0-1eda-4a70-9d6a-f335f264b6d7"
   },
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"entity_value\": \"وول ستريت\",\\n  \"entity_type\": \"ORG\"\\n}\\n```'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 70
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "id": "10-p7o_-snCf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "outputId": "1e0708ee-4c17-4ebf-ddd1-5ab3ceaee887"
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "error",
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-161840406.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unlsoth Finetuning"
   ],
   "metadata": {
    "id": "kJxCfYpa_l4l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install protobuf==5.29.5"
   ],
   "metadata": {
    "id": "EEd0NxzwED-z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d52e502f-b867-421b-9469-dabaaa26c87f"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: protobuf==5.29.5 in /usr/local/lib/python3.12/dist-packages (5.29.5)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U bitsandbytes"
   ],
   "metadata": {
    "id": "T_0Rwn2YLt5g"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -qU unsloth"
   ],
   "metadata": {
    "id": "I5gjfuh_6BbQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0029bbab-ee0b-493f-eaec-128549a852dd"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.7/131.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = load_from_disk(\"/content/drive/MyDrive/Wojood_NER/datasets/train_dataset\")\n",
    "val_dataset = load_from_disk(\"/content/drive/MyDrive/Wojood_NER/datasets/val_dataset\")"
   ],
   "metadata": {
    "id": "UVWb4KChig0h"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "id": "k5_ibp4O91Xa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "98feb4a7-8e4b-45f0-c17b-b0a146f9b6a7"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3383049919.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377,
     "referenced_widgets": [
      "e29674e83ba2456b9876ab6b3b25feae",
      "de9782d5ffd04a14b2c3e2ac048e3c31",
      "1e532b42f0454c9a98638757d7c3c7c5",
      "df92c075bf994f2d98439b546c7f5c32",
      "31bb5fe2a3344c3d805cce40aad37309",
      "2ebfc3443bd244409016fbafa910677e",
      "75ba8d46631240b48447aa1939246137",
      "7be2c48ebba047d58722abf8d826f9ea",
      "8a4acc82d25141d49750dcbbb4a64754",
      "7b43f92a07b041ccbef5e3e0d13f69ca",
      "8b3a82411a274d37b4a7b12b27fc7d3d",
      "d7ffb3783b77451b8fa3ddb413a75006",
      "87d97fc478704b4ea1a8dc53dbab6ca3",
      "5eb7278b9d1b415b9ccaf3c8e3aa5f69",
      "60c5d91d7b5f45e3b6ce18f70de55e1a",
      "a643509526f84702a95d3709b1265e80",
      "add0e989ea8443c9b66d6024b9f49205",
      "3c5e66c8fd924197ba2228b16afad3e0",
      "482d1bcc8e2d47fe995b2ec67bdcf7af",
      "7c27ed801b7f4257b770509d485b8e9c",
      "78a72f8608a645dcb7470db9e94e43db",
      "7cbf4b525b4e4f43ae489a4ad39987ef",
      "8057773439c34e2ea6c714e52212506f",
      "e112d708a7bd4a219459be6eaeedc3aa",
      "567050b5ef554e79bfdc63817104a8a2",
      "032ed6ed8ce642c781291bd270aaf35e",
      "1369414bbc2946d0be15064098d08be3",
      "64e38c4801b6490cbb1b7b15bd13de7a",
      "04e18578e345433f9b6a2ba8f6b1c737",
      "6349af938ba64f9291b497f9ec661b26",
      "265af2bf0fab4264829153f0f90ee39b",
      "d2d42412a20f4ebd8a477077d8df373c",
      "bc8bb3a6a5c0426782b377522ff5f0bc",
      "4a5f2bd6f0f44ebf837881c17e424012",
      "3e127992b4a44752b18e48c07b5fd55f",
      "e80f5141f68e4f638e163f514aa2f625",
      "db5adf4136304712b9c2755c1e43559b",
      "b6437a729efb48dfb178506bb9ecf769",
      "adaf8332f1064fda8bf3a59d3b69469c",
      "c74bf050b99d4e47b18baa1e6b663bb9",
      "4c9bbe5a044540c0929ed719b133d51d",
      "dc3e78f5c7b746bf9a754a295f782f42",
      "02b6089773484ae9bb1490edecd4b2f7",
      "69260d81f4f64b44b2320c88ec81291c",
      "ca3109216fe246099f1fd7ab04ad3ba9",
      "87c9626262c54264a19854b7a89b39fe",
      "430fd914a91c407f93c89e07dcc23eab",
      "8b802516716b4117bb634f8ade80058c",
      "22271604f80c4a4580bb1c44d17b1bc5",
      "1d792519949c43ae935cf182aae974b6",
      "1265e02db2084e81bc040a98cad2a744",
      "e3968a1711e34106957b2606a4c94869",
      "38e0f5156aa04716b5cadc5b1be0102a",
      "4bb9dffc3096434b9bd066ae91ae8004",
      "b29c0a0be1934483b101db988aab581e",
      "00df6818505749f694d7e5ef75bfabc5",
      "607114f4594a4e7f9d5ecc1540e04ed9",
      "f29381cdfc544dbfb310db0d039d5821",
      "e3764486950d4891a2720474dd06f712",
      "0717fd29e2dd459faf7ae5b4d60723b6",
      "dd9981feeca04a02a3813cf3a9a2ee8b",
      "b1e7c04240d74752b950c931005f8fa7",
      "600d83113d644cd6a09b8bb3d651187a",
      "3eb0d71b69ca4b8181787a64ee7500d3",
      "866ebfd9cb8f455aaeec5f232a5b3eb4",
      "8ad0e9010a7f43cabb3f11e2cba03807",
      "76646c6e01624b7fb5750943ceb79c7a",
      "d3f018e1c08c4e7e917989aec13da9cb",
      "dac3087936b341c79081d8be4174edf5",
      "73ef3ce403704e53a5d43a4701fca4f4",
      "24924b244dda428f903d433cc17a8a0d",
      "ee67f672407143c3b57073723f93c33d",
      "efed87e3684042f38c166621acc9da1f",
      "cdfff69e5e15458a9777ecda363a5488",
      "4c0b81a999554464b8a118edf4b869ed",
      "b782500906fb45c09ad05965bf6a68b1",
      "eb2badaee90b4a20ad0d104fdfe4ab5d",
      "45a6e2d57dd544788b03fe69cf826aff",
      "8f94b6424a004643b8814ab090de1438",
      "313359a8cc95473faca6eeae4e4ff0b7",
      "badeb0b89dd8408fb63567f1fdd66205",
      "f76f7653d283474b8c2b9aa632881029",
      "01142f1e0cae4e1ebed7da2eed9d43c6",
      "560efc1f6d7646d2bc0fbdfb8db63603",
      "3c639e06b99740829796c4d3ecefbf82",
      "bf854bc08c9c4c41a1a253a650d98a05",
      "c3ac137cd73942e59e295a48f75834de",
      "441bef0de1db4b46a8dedcb4311e5588"
     ]
    },
    "id": "g8CYQzIEAfNP",
    "outputId": "0a400ee0-7ee0-4828-ec7d-db7c456e5ed9"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.9.2: Fast Qwen2 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = False,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = 2048,\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")"
   ],
   "metadata": {
    "id": "74GaFnWrCeIA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b004dead-6f25-4360-ce25-4575eda8149f"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.9.2 patched 24 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        per_device_eval_batch_size = 8,\n",
    "        gradient_accumulation_steps = 4,\n",
    "\n",
    "        num_train_epochs = 1,\n",
    "        warmup_steps = 10,\n",
    "\n",
    "        learning_rate=7e-4,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "\n",
    "        weight_decay=0.05,\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 1,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 10,\n",
    "        logging_steps = 10,\n",
    "\n",
    "        output_dir = \"/content/drive/MyDrive/Wojood_NER/model_checkpoints\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        seed = 3407,\n",
    "        report_to=\"tensorboard\",\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "id": "CaktIqXrCeFw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "46340f41ba0047cc8a4742b03243e22f",
      "3ff93211942f4ce98d1c9231f19f348e",
      "ffc720b7c6ba48dcb30938a49b2e8285",
      "0560921844ad4aecb5d6d2d07c325f93",
      "1c767a27a0de4f1180841f7e4f6d8b74",
      "5d5b2e4f2a85499d9c72e4f7c2f7852d",
      "d416f965bbeb489f96ee6864908c391f",
      "4f117da8702a4e6696bf9a9baeea7c91",
      "e77909e4f924471dbe9d9b0fc3b06108",
      "3f0bc197677d4a87af9b282682aa9366",
      "70c348e55f604265b82a99ea3fdb3801",
      "a9c27df771fe481a8416f5f2abc0a722",
      "6d3e0cdd56784f3bb12eaf4869cb5fbd",
      "15f3dd4284ef4339b62ca1727433cb55",
      "47a82ea1c061466dad529e987ae9ada9",
      "0fc370402dbe44b88ccc0e471bc98d7a",
      "84c325ed6d734187bd4f57966dd0bb46",
      "7013d8ad3e824319a9a9376b4504e134",
      "760e8ca9589d458bbd2eeb6d1e42c576",
      "6fbdb75201a34670b694d6ead7cd2365",
      "b1a65a1589b8491fa390f28eab6b5ddb",
      "ee654c8a992f4933aa5a678662a078f4"
     ]
    },
    "outputId": "06644919-1d4f-4fbf-84fa-703fd500faad"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/1081 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "qaOLhSpICeB4",
    "outputId": "4ccb72aa-3757-426a-a7db-020aa5e85881"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,081 | Num Epochs = 1 | Total steps = 68\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 8,798,208 of 502,830,976 (1.75% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 04:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>0.274811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.316290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.238532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.225294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.217651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.210324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=68, training_loss=0.3262525937136482, metrics={'train_runtime': 295.0887, 'train_samples_per_second': 3.663, 'train_steps_per_second': 0.23, 'total_flos': 1396413322621440.0, 'train_loss': 0.3262525937136482, 'epoch': 1.0})"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/Wojood_NER/model\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/Wojood_NER/model\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cD-BgBUuAfEp",
    "outputId": "ee966428-659e-432c-d82e-dac3bc76d452"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Wojood_NER/model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/chat_template.jinja',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/vocab.json',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/merges.txt',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/added_tokens.json',\n",
       " '/content/drive/MyDrive/Wojood_NER/model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import userdata\n",
    "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
    "\n",
    "model.push_to_hub(\"AhmedNabil1/arabic_ner_qwen_model\", token=HUGGINGFACE_API_KEY)\n",
    "tokenizer.push_to_hub(\"AhmedNabil1/arabic_ner_qwen_model\", token=HUGGINGFACE_API_KEY)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "ca6db7e0ea1e486f90c859c10fae5755",
      "d193981fe87b4f5fba711252293cb82e",
      "87cf205da7094958bf496d9601d0e257",
      "9e4d126d50394d189bdf0aafc55a7be6",
      "b81457e6587b4833b133b48d40dfb9e1",
      "448e7eb73b534b5189d2ced0307f4c93",
      "8e4f55fc4ffb45518f98979df9b26607",
      "9432d47d31fc463bafc5d16cb87313c4",
      "eb47b3a704694a0583f3f1ae0c15237e",
      "1415b260afc84f33a5d40583c5cf4d41",
      "7a73123eaac04725863de587be78e44e",
      "e5ff4f20aeaa475ba75efdfc887e8fa3",
      "3ade1c69b2044228b5d5160271c09213",
      "c70f914cfce1430ca3dbf335b9cb537a",
      "859e33a057ab49479bca38af8a5a3ce7",
      "27b7ea5237624406b3fc44bd912ecff0",
      "c9375d77e21d459bbdb77ca51f1ed762",
      "dc6f5a439a9d44ef983dd5b9a645a955",
      "344c4f64569e43b486b3e2676f8ab3cf",
      "17b80497bffd49c8b35d608548fe9bc1",
      "4e31927cbb7e4b6c9191fe2cd011738c",
      "8ed5441315504c4ebdc3ca1c72b95f3e"
     ]
    },
    "id": "3WVL8pFAnLDv",
    "outputId": "88c686c3-ee32-4fcb-e01d-fbf022b6b89c"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/35.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved model to https://huggingface.co/AhmedNabil1/arabic_ner_qwen_model\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "id": "g5DRk1DSDX5i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_path = \"/content/drive/MyDrive/Wojood_NER/model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "metadata": {
    "id": "IDVZpPKFlrqI"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_path,\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.for_inference(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iyTXkfjAe-4",
    "outputId": "872ae1a1-3ad9-4763-e1ba-78fdff82717b"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.9.2: Fast Qwen2 patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_entities_from_story(story, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract named entities from Arabic text using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        story (str): The Arabic text to extract entities from\n",
    "        model: The fine-tuned model\n",
    "        tokenizer: The tokenizer\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response containing extracted entities in JSON format\n",
    "    \"\"\"\n",
    "    # Create the messages for entity extraction\n",
    "    entities_extraction_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"You are an advanced NLP entity extraction assistant.\",\n",
    "                \"Your task is to extract named entities from Arabic text according to a given Pydantic schema.\",\n",
    "                \"Ensure that the extracted entities exactly match how they appear in the text, without modifications.\",\n",
    "                \"Follow the schema strictly, maintaining the correct entity types and structure.\",\n",
    "                \"Output the extracted entities in JSON format, structured according to the provided Pydantic schema.\",\n",
    "                \"Do not add explanations, introductions, or extra text, Only return the formatted JSON output.\"\n",
    "            ])\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\\n\".join([\n",
    "                \"## Text:\",\n",
    "                story.strip(),\n",
    "                \"\",\n",
    "                \"## Pydantic Schema:\",\n",
    "                json.dumps(\n",
    "                    NERData.model_json_schema(), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "                \"\",\n",
    "                \"## Text Entities:\",\n",
    "                \"```json\"\n",
    "            ])\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        entities_extraction_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize and generate\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=False,\n",
    "        top_k=None,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ],
   "metadata": {
    "id": "1fRGZWvPbWXS"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "المحكمة العليا الإسرائيلية تقرر استمرار عمل الكسّارات الإسرائيلية في الضفة الغربية بتاريخ ( 29 / 12 / 2011 ) .\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "AnTUbEtKbWXT",
    "outputId": "adedcf8a-41fa-4e2d-e63b-dc5b4d100241"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"story_entities\":[{\"entity_value\":\"المحكمة العليا الإسرائيلية\",\"entity_type\":\"ORG\"},{\"entity_value\":\"فلسطيني\",\"entity_type\":\"NORP\"},{\"entity_value\":\"بتاريخ ( 29 / 12 / 2011 )\",\"entity_type\":\"DATE\"}]}'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTO6BH0NbWXT",
    "outputId": "2d23e34a-deae-4875-ef93-77c9c5ec1b93"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'story_entities': [{'entity_value': 'المحكمة العليا الإسرائيلية',\n",
       "   'entity_type': 'ORG'},\n",
       "  {'entity_value': 'فلسطيني', 'entity_type': 'NORP'},\n",
       "  {'entity_value': 'بتاريخ ( 29 / 12 / 2011 )', 'entity_type': 'DATE'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "مضابط بلدية نابلس عام ( 1308 ) هجري مضبط رقم 435 .\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "jo6UZ8DPbWXT",
    "outputId": "228de92a-a4f0-4061-8ad1-762ce76e1383"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"story_entities\":[{\"entity_value\":\"بلدية نابلس\",\"entity_type\":\"ORG\"},{\"entity_value\":\"نابلس\",\"entity_type\":\"GPE\"},{\"entity_value\":\"عام ( 1308 ) هجري\",\"entity_type\":\"DATE\"},{\"entity_value\":\"435\",\"entity_type\":\"ORDINAL\"}]}'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhaLzzc1bWXT",
    "outputId": "c66e78fc-5483-42a1-a9e1-b7b33ea6a245"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'story_entities': [{'entity_value': 'بلدية نابلس', 'entity_type': 'ORG'},\n",
       "  {'entity_value': 'نابلس', 'entity_type': 'GPE'},\n",
       "  {'entity_value': 'عام ( 1308 ) هجري', 'entity_type': 'DATE'},\n",
       "  {'entity_value': '435', 'entity_type': 'ORDINAL'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "story = \"\"\"\n",
    "تتجه أنظار \"وول ستريت\" إلى إنفيديا، سهم شركة الرقائق الرائدة الذي كان رمزاً لطفرة الذكاء الاصطناعي والمفضل لدى المستثمرين الأفراد، قبل تقرير أرباح الشركة، بعد الجرس يوم الأربعاء.\n",
    "\"\"\"\n",
    "\n",
    "response = extract_entities_from_story(story, model, tokenizer)\n",
    "response"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "egbqIc_jbWXU",
    "outputId": "0ae3c80d-5634-4c0f-ecbf-6f88e0f9781a"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'{\"story_entities\":[{\"entity_value\":\"وول ستريت\",\"entity_type\":\"ORG\"},{\"entity_value\":\"إنفيديا\",\"entity_type\":\"ORG\"},{\"entity_value\":\"الذين يُعرفون باسم ذوي الأقلية\",\"entity_type\":\"NORP\"}]}'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "json.loads(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVFEHCicbWXU",
    "outputId": "61ab7b3e-1f11-4451-a8ad-9adae5ce559e"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'story_entities': [{'entity_value': 'وول ستريت', 'entity_type': 'ORG'},\n",
       "  {'entity_value': 'إنفيديا', 'entity_type': 'ORG'},\n",
       "  {'entity_value': 'الذين يُعرفون باسم ذوي الأقلية', 'entity_type': 'NORP'}]}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "2E8EWFHcbX5k"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}